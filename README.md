# ðŸš¢ Titanic Survival Prediction using Neural Networks

A deep learning project that predicts passenger survival on the Titanic using a neural network built with TensorFlow and Keras.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.0+-orange.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

---

## ðŸ“‹ Table of Contents
- [Overview](#overview)
- [Dataset](#dataset)
- [How I Built This Project](#how-i-built-this-project)
- [Model Architecture](#model-architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Results](#results)
- [Technologies Used](#technologies-used)
- [Project Structure](#project-structure)
- [Future Improvements](#future-improvements)
- [Contributing](#contributing)
- [License](#license)

---

## ðŸŽ¯ Overview

This project implements a **binary classification neural network** to predict whether a passenger survived the Titanic disaster based on various features like passenger class, sex, age, fare, etc.

The model achieves competitive accuracy using a simple 2-layer neural network architecture with proper data preprocessing and standardization.

---

## ðŸ“Š Dataset

The project uses the **Titanic dataset** from Seaborn, which contains information about passengers including:

- **Features**: `pclass`, `sex`, `age`, `sibsp`, `parch`, `fare`, `embarked`, `class`, `who`, `deck`, etc.
- **Target**: `survived` (0 = Did not survive, 1 = Survived)
- **Size**: ~891 passengers

### Dataset Source
```python
import seaborn as sns
dataset = sns.load_dataset('titanic')
```

---

## ðŸ› ï¸ How I Built This Project

### Step 1: **Data Loading**
I started by loading the Titanic dataset from a CSV file:
```python
dataset = pd.read_csv('titanic.csv')
```

### Step 2: **Data Preprocessing**
This was a crucial step to prepare the data for the neural network:

#### 2.1 Handling Missing Values
- Dropped columns with too many missing values: `age`, `embarked`
- These columns would have required imputation which could introduce bias

#### 2.2 Encoding Categorical Variables
- Converted categorical features to numerical using **one-hot encoding**:
  - `sex` â†’ `sex_male`, `sex_female`
  - `class` â†’ `class_First`, `class_Second`, `class_Third`
  - `who` â†’ `who_man`, `who_woman`, `who_child`
  - `deck` â†’ `deck_A`, `deck_B`, `deck_C`, etc.
- Used `drop_first=True` to avoid multicollinearity

#### 2.3 Feature Selection
- Dropped redundant columns: `alive`, `embark_town`, `adult_male`, `alone`
- These were either duplicates or not useful for prediction

### Step 3: **Data Splitting**
Split the dataset into training and testing sets:
- **Training set**: 80% (for learning patterns)
- **Testing set**: 20% (for evaluating performance)
- Used `random_state=42` for reproducibility

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

### Step 4: **Feature Scaling**
Applied **StandardScaler** to normalize features:
- Transforms features to have mean = 0 and standard deviation = 1
- Essential for neural networks to converge faster
- Prevents features with larger values from dominating

```python
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```

### Step 5: **Building the Neural Network**
Created a simple but effective architecture:

#### Architecture Design:
```
Input Layer  â†’  Hidden Layer (10 neurons, ReLU)  â†’  Output Layer (1 neuron, Sigmoid)
```

- **Input Layer**: 10 neurons with ReLU activation
  - ReLU helps with non-linearity and prevents vanishing gradients
  
- **Output Layer**: 1 neuron with Sigmoid activation
  - Sigmoid outputs probability between 0 and 1 (perfect for binary classification)

```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

### Step 6: **Model Compilation**
Configured the learning process:
- **Optimizer**: Adam (adaptive learning rate, works well for most problems)
- **Loss Function**: Binary Crossentropy (standard for binary classification)
- **Metrics**: Accuracy (to track performance)

```python
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
```

### Step 7: **Training the Model**
Trained the model with:
- **Epochs**: 88 (number of complete passes through the dataset)
- **Batch Size**: 32 (number of samples per gradient update)
- **Verbose**: 1 (to see training progress)

```python
model.fit(X_train, y_train, epochs=88, batch_size=32, verbose=1)
```

### Step 8: **Model Evaluation**
Evaluated the model on unseen test data:
```python
loss, accuracy = model.evaluate(X_test, y_test)
```

### Step 9: **Code Refactoring**
Organized the code into clean, reusable functions:
- `load_data()` - Data loading
- `preprocess_data()` - Data preprocessing
- `split_and_scale_data()` - Train/test split and scaling
- `build_model()` - Model architecture
- `train_model()` - Training logic
- `evaluate_model()` - Evaluation and results
- `main()` - Orchestrates the entire pipeline

---

## ðŸ§  Model Architecture

```
Model: "sequential"
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
input_layer (Dense)         (None, 10)                140       
output_layer (Dense)        (None, 1)                 11        
=================================================================
Total params: 151
Trainable params: 151
Non-trainable params: 0
_________________________________________________________________
```

### Why This Architecture?
- **Simple yet effective**: For a dataset of ~700 samples, a complex model would overfit
- **Fast training**: Only 151 parameters to learn
- **Good baseline**: Can be improved with additional layers or regularization

---

## ðŸ’» Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Step-by-Step Installation

1. **Clone the repository**
```bash
git clone https://github.com/manish930s/titanic-neural-network.git
cd titanic-neural-network
```

2. **Create a virtual environment** (recommended)
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

3. **Install required packages**
```bash
pip install numpy pandas seaborn matplotlib scikit-learn tensorflow
```

Or use a requirements file:
```bash
pip install -r requirements.txt
```

### Requirements.txt
```
numpy>=1.21.0
pandas>=1.3.0
seaborn>=0.11.0
matplotlib>=3.4.0
scikit-learn>=0.24.0
tensorflow>=2.6.0
```

---

## ðŸš€ Usage

### Basic Usage

1. **Ensure you have the dataset**
   - Place `titanic.csv` in the project directory
   - Or update `DATASET_PATH` in the script

2. **Run the script**
```bash
python titanic-neural-network.py
```

### Expected Output
```
Loading dataset...
Preprocessing data...
Splitting and scaling data...
Building model...

Model Architecture:
Model: "sequential"
...

Training model...
Epoch 1/88
...
Epoch 88/88

Evaluating model...
==================================================
MODEL EVALUATION RESULTS
==================================================
Test Loss:     0.4523
Test Accuracy: 78.65%
==================================================
```

### Customization

You can modify hyperparameters at the top of the script:

```python
# Constants
DATASET_PATH = r'/content/titanic.csv'  # Update your dataset path
TEST_SIZE = 0.2                          # Train/test split ratio
RANDOM_STATE = 42                        # For reproducibility
EPOCHS = 88                              # Number of training epochs
BATCH_SIZE = 32                          # Batch size
HIDDEN_UNITS = 10                        # Neurons in hidden layer
```

---

## ðŸ“ˆ Results

### Model Performance
- **Test Accuracy**: ~78-82% (varies slightly due to random initialization)
- **Test Loss**: ~0.45-0.50

### Training Process
- The model typically converges within 50-60 epochs
- Training is fast (< 1 minute on most machines)
- No significant overfitting observed

### Confusion Matrix (Example)
```
                Predicted
              0         1
Actual  0    95        15
        1    24        45
```

---

## ðŸ› ï¸ Technologies Used

| Technology | Purpose |
|------------|---------|
| **Python** | Programming language |
| **NumPy** | Numerical computations |
| **Pandas** | Data manipulation and analysis |
| **Seaborn** | Dataset loading and visualization |
| **Matplotlib** | Plotting and visualization |
| **Scikit-learn** | Data preprocessing and splitting |
| **TensorFlow/Keras** | Neural network implementation |

---

## ðŸ“ Project Structure

```
titanic-neural-network/
â”‚
â”œâ”€â”€ titanic-neural-network.py    # Main script
â”œâ”€â”€ titanic.csv                   # Dataset (not included)
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ requirements.txt              # Python dependencies

```

---

## ðŸ”® Future Improvements

Here are some ideas to enhance this project:

### 1. **Model Improvements**
- [ ] Add more hidden layers (deeper network)
- [ ] Implement dropout for regularization
- [ ] Try different activation functions (LeakyReLU, ELU)
- [ ] Experiment with different optimizers (SGD, RMSprop)

### 2. **Data Improvements**
- [ ] Handle missing values with imputation instead of dropping
- [ ] Feature engineering (e.g., family size, title extraction)
- [ ] Cross-validation for more robust evaluation
- [ ] Data augmentation techniques

### 3. **Visualization**
- [ ] Plot training/validation loss curves
- [ ] Visualize feature importance
- [ ] Create confusion matrix heatmap
- [ ] ROC curve and AUC score

### 4. **Deployment**
- [ ] Create a web interface with Flask/Streamlit
- [ ] Save and load trained models
- [ ] API endpoint for predictions
- [ ] Docker containerization

### 5. **Comparison**
- [ ] Compare with other algorithms (Random Forest, SVM, XGBoost)
- [ ] Ensemble methods
- [ ] Hyperparameter tuning with GridSearch

---

## ðŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. Fork the repository
2. Create a new branch (`git checkout -b feature/improvement`)
3. Make your changes
4. Commit your changes (`git commit -am 'Add new feature'`)
5. Push to the branch (`git push origin feature/improvement`)
6. Create a Pull Request

---

## ðŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ðŸ‘¨â€ðŸ’» Author

**Manish**

- GitHub: [@yourusername](https://github.com/manish93s)
- LinkedIn: [Your LinkedIn](https://linkedin.com/in/manish2111)

---

## ðŸ™ Acknowledgments

- Dataset from [Seaborn](https://seaborn.pydata.org/)
- Inspired by the classic Kaggle Titanic competition
- TensorFlow and Keras documentation

---

## ðŸ“š Learning Resources

If you're new to deep learning, here are some helpful resources:

- [TensorFlow Official Tutorials](https://www.tensorflow.org/tutorials)
- [Keras Documentation](https://keras.io/)
- [Deep Learning Specialization by Andrew Ng](https://www.coursera.org/specializations/deep-learning)
- [Fast.ai Course](https://www.fast.ai/)

---

## â­ Star this repo if you found it helpful!

**Happy Learning! ðŸš€**
